---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---
![prof](../assets/images/prof2.jpg){: width="20%" height="auto"}
![brrrr](../assets/images/brrrr.jpg){: width="250" height="auto"}

dicksonb [at] iu [dot] edu

Hi, I'm a PhD student in computer science at Indiana University working with [Zoran Tiganj](https://homes.luddy.indiana.edu/ztiganj/).

Interests:

 - attention, memory, long-context modeling, hybrid models, cognitive science

More Interests:

 - llm training methodology, architecture, open models, distributed training, retrieval 
 - prepretraining, pretraining, postprepreposttraining, mid-training

Previously: 

I did a master's in computational linguistics at Indiana University with [Damir Cavar](https://damir.cavar.me/) in the [NLP Lab](https://nlp-lab.org/) working on time and event reasoning, and applied NLP with knowledge graphs and ontologies, and before that did my bachelor's at Michigan State University where I studied linguistics, TESOL, Chinese, and Korean, and did a summer language program at Harbin Institute of Technology.

---


Research:

**Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows** \[paper\] Dickson, B., & Tiganj, Z. (in press). Gradual forgetting: Logarithmic compression for extending transformer context windows. In First Workshop on CogInterp: Interpreting Cognition in Deep Learning Models (NeurIPS 2025). 

![prof](../assets/images/sith.png){: width="40%" height="auto"}


**Time-Local Transformer** [[paper]](https://link.springer.com/article/10.1007/s42113-025-00253-9)[[code]](https://github.com/cogneuroai/time-local-transformer) Dickson, B., Mochizuki-Freeman, J., Kabir, M.R., Tiganj, Z. Time-Local Transformer. Comput Brain Behav (2025). https://doi.org/10.1007/s42113-025-00253-9

![prof](../assets/images/timelocal.png){: width="40%" height="auto"}


**Comparing Perceptual Judgments in Large Multimodal Models and Humans** [[paper]](https://link.springer.com/article/10.3758/s13428-025-02728-w)[[code]](https://github.com/cogneuroai/multimodal-models-rock)[[website]](https://cognlp.com) Dickson, B.\*, Maini, S. S.\*, Sanders, C., Nosofsky, R., & Tiganj, Z. Comparing Perceptual Judgments in Large Multimodal Models and Humans. Behavior Research Methods 57, 203 (2025). https://doi.org/10.3758/s13428-025-02728-w

![prof](../assets/images/rock.png){: width="40%" height="auto"}


---

More Research:

**Advancing Adverse Drug Event Detection in Social Media Through Knowledge Graph and GraphRAG LLM Architectures** [[poster]](../assets/ade.pdf) Davis, A., Dickson, B., Cavar, D., Valdez, D., & Tyers, F. (2025). Advancing Adverse Drug Event Detection in Social Media Through Knowledge Graph and GraphRAG LLM Architectures [Poster presentation]. American Academy of Health Behavior (AAHB), San Diego, CA, United States.

**A Two-Stage NLP System for Extracting and Normalizing Adverse Drug Events from Tweets.** [[paper]](https://aclanthology.org/2024.smm4h-1.27.pdf) Davis, A., Dickson, B., & Kubler, S. (2024). A Two-Stage NLP System for Extracting and Normalizing Adverse Drug Events from Tweets. In Proceedings of the 9th Social Media Mining for Health Research and Applications (SMM4H 2024) Workshop and Shared Tasks, pages 117–120, Bangkok, Thailand. Association for Computational Linguistics. 

**Computing Ellipsis Constructions: Comparing Classical NLP and LLM Approaches** [[paper]](https://openpublishing.library.umass.edu/scil/article/id/2147/) Cavar, D., Tiganj, Z., Mompelat, L. V., & Dickson, B. Computing Ellipsis Constructions: Comparing Classical NLP and LLM Approaches. Society for Computation in Linguistics 7(1), 217–226 (2024). https://doi.org/10.7275/scil.2147

**Event sequencing annotation with TIE-ML** [[paper]](https://aclanthology.org/2022.isa-1.5/) Cavar, D., Aljubailan, A., Mompelat, L., Won, Y., Dickson, B., Fort, M., Davis, A., & Kim, S. (2022). Event sequencing annotation with TIE-ML. In Proceedings of the 18th Joint ACL - ISO Workshop on Interoperable Semantic Annotation within LREC2022 (pp. 33–41). Marseille, France: European Language Resources Association.

**Temporal Information and Event Markup Language: TIE-ML Markup Process and Schema Version 1.0** [[paper]](https://www.thinkmind.org/index.php?view=article&articleid=semapro_2021_1_60_30029) Cavar, D., Dickson, B., Aljubailan, A., & Kim, S. (2021). Temporal Information and Event Markup Language: TIE-ML Markup Process and Schema Version 1.0. In Proceedings of the 15th International Conference on Advances in Semantic Processing (SEMAPRO 2021), pages 29-36, Barcelona, Spain.

---

More More Interests:

 -  formula 1, racket sports, video games, card games, indycar, kalaOK, honda civic eg6, weightlifting, gojo 

--- 


Teaching:

Fall 2025, Assistant Instructor, Applied Machine Learning (Indiana University)

Summer 2024, [Generative AI and Symbolic Knowledge Representations: Large Language Models, Knowledge, and Reasoning](https://damir.cavar.me/ESSLLI24_LLM_KG.github.io/) (ESSLLI 2024, Leuven, Belgium)

Spring 2022, Fall 2023, Spring 2024, Fall 2024, Spring 2025, Assistant Instructor, Data Mining (Indiana University)

Spring 2020, Adult Communicative Focused English (Michigan State University)

---


Education:

PhD, Computer Science, Cognitive Science, Indiana University, ~2027


MS, Computer Science, Indiana University, 2025


MS, Computational Linguistics, Indiana University, 2022


BA, Linguistics, Michigan State University, 2020
